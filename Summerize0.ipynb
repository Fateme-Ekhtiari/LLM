{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summerize the text"
      ],
      "metadata": {
        "id": "iTWXKbiwIFlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for using huggingface datasets\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "aFOnZtTTIL3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "I99jZigmILxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Summary\n",
        "DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics."
      ],
      "metadata": {
        "id": "mA8_UL1BJCvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset of huggingface dialouge and base human summary\n",
        "\n",
        "huggfaceDataset = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggfaceDataset)"
      ],
      "metadata": {
        "id": "WlU1JT2yILuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  check dialogsum dataset"
      ],
      "metadata": {
        "id": "EyUImaCnKjJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# name of columns\n",
        "dataset.column_names"
      ],
      "metadata": {
        "id": "DK95nzQiILrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of rows\n",
        "dataset.num_rows"
      ],
      "metadata": {
        "id": "luW0FSUrILoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of columns\n",
        "dataset.num_columns"
      ],
      "metadata": {
        "id": "quIvTRsLKYTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "jUO4ikuxKYQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check sample of 999 of dataset\n",
        "print(dataset['train'][999]['dialogue'])"
      ],
      "metadata": {
        "id": "-AVIseU-JttF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][999]['summary'])"
      ],
      "metadata": {
        "id": "dxuemrR6K5Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][999]['topic']"
      ],
      "metadata": {
        "id": "c731hTgOK5Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample of 500 and 1200\n",
        "example_indices = [500, 1200]\n",
        "\n",
        "dash_line = '*'.join('' for x in range(100))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1 ,'\\n')\n",
        "    print(dash_line)\n",
        "    print('INPUT DIALOGUE:','\\n')\n",
        "    print(dataset['test'][index]['dialogue'],'\\n')\n",
        "    print(dash_line)\n",
        "    print('BASELINE HUMAN SUMMARY:','\\n')\n",
        "    print(dataset['test'][index]['summary'],'\\n')\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "id": "GXRgZyMgK5D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)\n",
        "\n",
        "## Overview\n",
        "FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models - it is an enhanced version of T5 that has been finetuned in a mixture of tasks."
      ],
      "metadata": {
        "id": "Qkd7f0AYNRnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of FLAN-T5 from huggingface\n",
        "\n",
        "The goal of this code is to use a pre-trained sequence-to-sequence language model to generate text — specifically, to complete or continue a given text prompt as a helpful assistant"
      ],
      "metadata": {
        "id": "mnAe1jNCQu_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\" ,return_tensors = 'pt')\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "86C_dwkcNAoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now step by step explain how to use this model to summerize.\n",
        "\n"
      ],
      "metadata": {
        "id": "UHKzj-hZRRIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model flan-t5 base\n",
        "\n",
        "\n",
        "model_name='google/flan-t5-base'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "60nfC9WDRHW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the tokenizer for the **FLAN-T5** model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
      ],
      "metadata": {
        "id": "2BYwRVSIRqXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "kZQsseaxRO5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Text Input → Tokenization → Numerical Encoding → Vector Representation → Decoding → Text Output`\n",
        "\n",
        "\n",
        "## Detailed Technical Breakdown\n",
        "\n",
        "1. Text Acquisition & Preprocessing\n",
        "\n",
        "Input Reception: Capture raw textual input from source (user query, document, API request)\n",
        "\n",
        "Normalization: Standardize casing, remove extraneous whitespace, handle special characters\n",
        "\n",
        "Sanitization: Filter inappropriate content, validate input boundaries\n",
        "\n",
        "2. Tokenization Phase\n",
        "\n",
        "Segmentation: Divide continuous text into discrete linguistic units (tokens)\n",
        "\n",
        "Methodology: Employ subword tokenization (e.g., WordPiece, Byte-Pair Encoding)\n",
        "\n",
        "Special Tokens: Insert control tokens ([CLS], [SEP], [PAD]) for model-specific processing\n",
        "\n",
        "3. Numerical Encoding\n",
        "\n",
        "Vocabulary Mapping: Convert each token to corresponding integer ID from pretrained vocabulary\n",
        "\n",
        "Vector Creation: Generate tensor representation for batch processing\n",
        "\n",
        "Attention Masks: Create binary masks distinguishing actual tokens from padding\n",
        "\n",
        "4. Model Processing (Vector Operations)\n",
        "\n",
        "Embedding Lookup: Convert token IDs to dense vector representations\n",
        "\n",
        "Neural Transformation: Apply transformer architecture (self-attention, feed-forward layers)\n",
        "\n",
        "Contextualization: Generate context-aware representations via multi-head attention\n",
        "\n",
        "5. Decoding & Text Reconstruction\n",
        "\n",
        "Token Generation: Produce output token IDs through autoregressive sampling\n",
        "\n",
        "Detokenization: Map numerical IDs back to string tokens\n",
        "\n",
        "Post-processing: Remove special tokens, reconstruct original formatting"
      ],
      "metadata": {
        "id": "FB5oMeHVSVjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence\n",
        "txt = \"Test for encoding and decoding ?\"\n",
        "\n",
        "#tokenize the sentence\n",
        "txt_encoded = tokenizer(txt , return_tensors='pt')\n",
        "\n",
        "#decode the sentence\n",
        "txt_decoded = tokenizer.decode(txt_encoded[\"input_ids\"][0] ,\n",
        "                               skip_special_tokens=True)\n",
        "\n",
        "print('encoded text:')\n",
        "print(txt_encoded['input_ids'][0])\n",
        "print('\\ntxt_decoded:')\n",
        "print(txt_decoded)\n",
        "\n"
      ],
      "metadata": {
        "id": "CN8QpIXzTow_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_encoded"
      ],
      "metadata": {
        "id": "8JKtcwgHXnuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With 3 sentences in a batch\n",
        "sentences = [\"What time is it?\", \"Hello world\", \"How are you?\"]\n",
        "batch_encoded = tokenizer(sentences, return_tensors='pt', padding=True)\n",
        "\n",
        "print(batch_encoded[\"input_ids\"])\n",
        "# tensor([[  101,  2054,  2051,  2003,  2009,  1029,   102,     0,     0],\n",
        "#         [  101,  7592,  2088,   102,     0,     0,     0,     0,     0],\n",
        "#         [  101,  2129,  2024,  2017,  1029,   102,     0,     0,     0]])\n",
        "\n",
        "# Access each sentence separately:\n",
        "print(batch_encoded[\"input_ids\"][0])  # First sentence token IDs\n",
        "print(batch_encoded[\"input_ids\"][1])  # Second sentence token IDs\n",
        "print(batch_encoded[\"input_ids\"][2])  # Third sentence token IDs"
      ],
      "metadata": {
        "id": "L8u4pfbvZ9-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It's time to explore how well the base LLM summarize dilogues without any prompt enginnering."
      ],
      "metadata": {
        "id": "mf1Q5BKQXl__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [500, 1200]\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line,'\\n')\n",
        "    print('Example ', i + 1,'\\n')\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{dialogue}','\\n')\n",
        "    print(dash_line,'\\n')\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}','\\n')\n",
        "    print(dash_line,'\\n')\n",
        "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n','\\n')"
      ],
      "metadata": {
        "id": "LKEYSOqpV3hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt engineering"
      ],
      "metadata": {
        "id": "j8h5KRx5j5H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "\n",
        "  dialogue = dataset['test'][index]['dialogue']\n",
        "\n",
        "  summary = dataset['test'][index]['summary']\n",
        "\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "\n",
        "  Summarize the following conversation.\n",
        "\n",
        "  {dialogue}\n",
        "\n",
        "  Summary:\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "  outputs = tokenizer.decode(model.generate(inputs[\"input_ids\"] ,\n",
        "                                            max_new_tokens =50,)[0],\n",
        "                             skip_special_tokens=True)\n",
        "  print(dash_line,'\\n')\n",
        "  print('Example ', i + 1,'\\n')\n",
        "  print(dash_line,'\\n')\n",
        "  print(f'INPUT PROMPT:\\n{prompt}','\\n')\n",
        "  print(dash_line,'\\n')\n",
        "  print(f'BASELINE HUMAN SUMMARY:\\n{summary}','\\n')\n",
        "  print(dash_line,'\\n')\n",
        "  print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n','\\n')\n"
      ],
      "metadata": {
        "id": "HgVGf6k6V3df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Inference with the Prompt Template from FLAN-T5"
      ],
      "metadata": {
        "id": "mbW3VWwXpE9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,index in enumerate(example_indices):\n",
        "\n",
        "  dialogue = dataset['test'][index]['dialogue']\n",
        "  summary = dataset['test'][index]['summary']\n",
        "\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Dialogue:\n",
        "  {dialogue}\n",
        "  what was going on?\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "  outputs = tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
        "                                            max_new_tokens=50)[0],\n",
        "                             skip_special_tokens = True)\n",
        "\n",
        "\n",
        "  print(dash_line , '\\n')\n",
        "  print('Example ', i+1, '\\n')\n",
        "  print(dash_line, '\\n')\n",
        "  print(f'input prompt :\\n{prompt}')\n",
        "  print(dash_line, '\\n')\n",
        "  print(f'base line human summary:\\n{summary}\\n')\n",
        "  print(dash_line, '\\n')\n",
        "  print(f'model generation - zero shot: \\n{outputs}\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mkfvkd5-V3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 4 -  One Shot Inference with the Prompt Template from FLAN-T5"
      ],
      "metadata": {
        "id": "VewMOlCowMh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(example_indices_full, example_index_to_summarize):\n",
        "\n",
        "  prompt=''\n",
        "\n",
        "  for index in example_indices_full:\n",
        "      dialogue = dataset['test'][index]['dialogue']\n",
        "      summary = dataset['test'][index]['summary']\n",
        "\n",
        "      prompt += f\"\"\"\n",
        "\n",
        "Dialogue:\n",
        "{dialogue}\n",
        "\n",
        "what was going on?\n",
        "{summary}\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "      prompt += f\"\"\"\n",
        "\n",
        "Dialogue:\n",
        "{dialogue}\n",
        "\n",
        "what was going on?\n",
        "\"\"\"\n",
        "      return prompt\n"
      ],
      "metadata": {
        "id": "CXthKGyCw11i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "one_shot_prompt = build_prompt(example_indices_full, example_index_to_summarize)\n",
        "print(f'one shot prompt: {one_shot_prompt}')"
      ],
      "metadata": {
        "id": "l2jJWPesyqxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "outputs = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],\n",
        "                           skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(dash_line ,'\\n')\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n','\\n')\n",
        "print(dash_line,'\\n')\n",
        "print(f'MODEL GENERATION - ONE SHOT:\\n{output}','\\n')"
      ],
      "metadata": {
        "id": "9x8tCO0nzcB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Inference with the Prompt Template from FLAN-T5"
      ],
      "metadata": {
        "id": "FsZCISg1wZci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40, 80, 120]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "few_shot_prompt = build_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(few_shot_prompt)"
      ],
      "metadata": {
        "id": "dCFcNGVU0kLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line,'\\n')\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n','\\n')\n",
        "print(dash_line,'\\n')\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}','\\n')"
      ],
      "metadata": {
        "id": "sb18_6qK0uZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}